{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/geochat/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-07 22:17:39,155] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os,json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from changechat.model.multimodal_encoder.clip_encoder import CLIPVisionTower\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, mm_hidden_size, hidden_size=None, img_feature_w=18, img_feature_h=18\n",
    "    ):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.d_model = mm_hidden_size\n",
    "\n",
    "        encoder_self_layer_classifier = nn.TransformerEncoderLayer(\n",
    "            2 * self.d_model, nhead=2, dim_feedforward=int(2 * self.d_model)\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_self_layer_classifier, num_layers=1\n",
    "        )\n",
    "        self.w_embedding = nn.Embedding(img_feature_w, int(self.d_model / 2))\n",
    "        self.h_embedding = nn.Embedding(img_feature_h, int(self.d_model / 2))\n",
    "        self.classifier_head = nn.Linear(2 * self.d_model, 2)\n",
    "\n",
    "        # cls_token\n",
    "        scale = self.d_model**-0.5\n",
    "        self.cls_changeflag = nn.Parameter(scale * torch.randn(1, 2 * self.d_model))\n",
    "\n",
    "    def position_embedding_2D_func(self, img_feat_A, img_feat_B):\n",
    "        device = img_feat_A.device\n",
    "        batch = img_feat_B.shape[0]\n",
    "        Len_feat = img_feat_B.shape[1]\n",
    "        h = int(math.sqrt(Len_feat))\n",
    "        w = h\n",
    "        pos_w = torch.arange(w, device=device).to(device)\n",
    "        pos_h = torch.arange(h, device=device).to(device)\n",
    "        embed_w = self.w_embedding(pos_w)\n",
    "        embed_h = self.h_embedding(pos_h)\n",
    "        position_embedding = torch.cat(\n",
    "            [\n",
    "                embed_w.unsqueeze(0).repeat(h, 1, 1),\n",
    "                embed_h.unsqueeze(1).repeat(1, w, 1),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        position_embedding = position_embedding.unsqueeze(0).repeat(\n",
    "            batch, 1, 1, 1\n",
    "        )  # (batch, h, w, d_model)\n",
    "        position_embedding = position_embedding.view(batch, -1, self.d_model)\n",
    "        img_feat_A = img_feat_A + position_embedding  # NLD\n",
    "        img_feat_B = img_feat_B + position_embedding  # NLD\n",
    "\n",
    "        return img_feat_A, img_feat_B\n",
    "\n",
    "    def forward(self, img_feat):\n",
    "        img_feat_A = img_feat[:, 0, ...]  # (N,L,768)\n",
    "        img_feat_B = img_feat[:, 1, ...]  # (N,L,768)\n",
    "\n",
    "        # 2D image position_embedding\n",
    "        img_feat_A, img_feat_B = self.position_embedding_2D_func(\n",
    "            img_feat_A, img_feat_B\n",
    "        )  # (N, L, D)\n",
    "\n",
    "        img_feat = torch.cat([img_feat_A, img_feat_B], dim=-1)  # (N, L, 2D)\n",
    "        img_feat_with_cls = torch.cat(\n",
    "            [\n",
    "                self.cls_changeflag.unsqueeze(0).expand(\n",
    "                    img_feat.shape[0], *self.cls_changeflag.shape\n",
    "                ),\n",
    "                img_feat,\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        img_feat_with_cls = self.transformer_encoder(\n",
    "            img_feat_with_cls.permute(1, 0, 2)\n",
    "        ).permute(\n",
    "            1, 0, 2\n",
    "        )  # (N, L, 2D)\n",
    "        change_pred = self.classifier_head(img_feat_with_cls[:, 0, :])\n",
    "        return change_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ChangeClassifierConfig:\n",
    "    mm_vision_tower = \"/root/autodl-tmp/GeoChat/hf-models/clip-vit-large-patch14-336\"\n",
    "    mm_vision_select_layer = -2\n",
    "\n",
    "class ChangeClassifier(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ChangeClassifier, self).__init__()\n",
    "        self.vision_tower = CLIPVisionTower(config.mm_vision_tower, config)\n",
    "        # freeeze vision_tower\n",
    "        self.vision_tower.requires_grad_(False)\n",
    "\n",
    "        if config.mm_vision_tower.endswith(\"clip-vit-large-patch14-336\"):\n",
    "            mm_hidden_size = 1024\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        self.classifier = Classifier(mm_hidden_size = mm_hidden_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # 提取两幅图像的特征\n",
    "        assert images.ndim == 5\n",
    "        concat_images = torch.cat([image for image in images], dim=0)\n",
    "        image_features = self.vision_tower(concat_images)\n",
    "        split_sizes = [image.shape[0] for image in images]\n",
    "        image_features = torch.split(\n",
    "            image_features, split_sizes, dim=0\n",
    "        )  # b tuples of [2, N, L]\n",
    "        image_features = torch.stack(image_features, dim=0)  # [b, 2, N, L]\n",
    "        change_pred = self.classifier(image_features)\n",
    "        return change_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataConfig:\n",
    "    data_path = \"/root/autodl-tmp/LEVIR-MCI-dataset/ChangeChat_classify.json\"\n",
    "    image_folder = \"/root/autodl-tmp/LEVIR-MCI-dataset/images\"\n",
    "    image_processor = None\n",
    "\n",
    "class LazySupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config\n",
    "    ):\n",
    "        super(LazySupervisedDataset, self).__init__()\n",
    "        list_data_dict = json.load(open(config.data_path, \"r\"))\n",
    "        self.list_data_dict = list_data_dict\n",
    "        self.processor = config.image_processor\n",
    "        self.image_folder = config.image_folder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_data_dict)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        sources = self.list_data_dict[i]\n",
    "        if isinstance(i, int):\n",
    "            sources = [sources]\n",
    "        assert len(sources) == 1, \"Don't know why it is wrapped to a list\"  # FIXME\n",
    "        image_file = self.list_data_dict[i][\"image\"]\n",
    "        image_folder = self.image_folder\n",
    "        processor = self.processor\n",
    "\n",
    "        if isinstance(image_file, str):\n",
    "            image_file_list = [image_file]\n",
    "        elif isinstance(image_file, list):\n",
    "            image_file_list = image_file\n",
    "\n",
    "        imageList = []\n",
    "        for _image_file in image_file_list:\n",
    "            image = Image.open(\n",
    "                (os.path.join(image_folder, _image_file)).strip()\n",
    "            ).convert(\"RGB\")\n",
    "            image = processor.preprocess(\n",
    "                image,\n",
    "                do_resize=True,\n",
    "                crop_size={\"height\": 252, \"width\": 252},\n",
    "                size={\"shortest_edge\": 252},\n",
    "                return_tensors=\"pt\",\n",
    "            )[\"pixel_values\"][0]\n",
    "            imageList.append(image)\n",
    "\n",
    "        data_dict = dict()\n",
    "        # 将多幅图像拼成一个Tensor\n",
    "        data_dict[\"images\"] = torch.stack(imageList, dim=0)  # (2, c, h, w)\n",
    "\n",
    "        # 如果有变化标签\n",
    "        if \"changeflag\" in self.list_data_dict[i]:\n",
    "            data_dict[\"change_labels\"] = torch.tensor(\n",
    "                self.list_data_dict[i][\"changeflag2\"]\n",
    "            )\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = ChangeClassifierConfig()\n",
    "model = ChangeClassifier(config)\n",
    "model = model.to(device)\n",
    "# 创建数据集和数据加载器\n",
    "dataset_config = DataConfig()\n",
    "dataset_config.image_processor = model.vision_tower.image_processor\n",
    "train_dataset = LazySupervisedDataset(dataset_config)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 验证集\n",
    "dataset_config.data_path = \"/root/autodl-tmp/GeoChat/load/Test_CC_gt.json\"\n",
    "test_dataset = LazySupervisedDataset(dataset_config)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "params = [v for v in model.parameters() if v.requires_grad]\n",
    "num_params = sum([v.numel() for v in model.parameters() if v.requires_grad])/1000000\n",
    "optimizer = optim.Adam(params, lr=1e-4)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhanlinwu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/autodl-tmp/GeoChat/changechat/train/wandb/run-20240807_221745-5jpag673</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hanlinwu/change_classifier/runs/5jpag673' target=\"_blank\">absurd-pond-3</a></strong> to <a href='https://wandb.ai/hanlinwu/change_classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hanlinwu/change_classifier' target=\"_blank\">https://wandb.ai/hanlinwu/change_classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hanlinwu/change_classifier/runs/5jpag673' target=\"_blank\">https://wandb.ai/hanlinwu/change_classifier/runs/5jpag673</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hanlinwu/change_classifier/runs/5jpag673?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fb135872ce0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"change_classifier\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def calculate_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(data_loader):\n",
    "            images, labels = batch['images'].to(device), batch['change_labels'].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'changeflag2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mcalculate_accuracy\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(data_loader):\n\u001b[1;32m      8\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchange_labels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n",
      "File \u001b[0;32m~/miniconda3/envs/geochat/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/geochat/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/geochat/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/geochat/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/geochat/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[9], line 58\u001b[0m, in \u001b[0;36mLazySupervisedDataset.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# 如果有变化标签\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchangeflag\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlist_data_dict[i]:\n\u001b[1;32m     57\u001b[0m     data_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchange_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m---> 58\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_data_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchangeflag2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     59\u001b[0m     )\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_dict\n",
      "\u001b[0;31mKeyError\u001b[0m: 'changeflag2'"
     ]
    }
   ],
   "source": [
    "val_accuracy = calculate_accuracy(model, test_loader, device)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/400], Step [10/213], Loss: 0.6896\n",
      "Epoch [1/400], Step [20/213], Loss: 0.8022\n",
      "Epoch [1/400], Step [30/213], Loss: 0.5419\n",
      "Epoch [1/400], Step [40/213], Loss: 0.6253\n",
      "Epoch [1/400], Step [50/213], Loss: 0.4691\n",
      "Epoch [1/400], Step [60/213], Loss: 0.3945\n",
      "Epoch [1/400], Step [70/213], Loss: 0.3566\n",
      "Epoch [1/400], Step [80/213], Loss: 0.3365\n",
      "Epoch [1/400], Step [90/213], Loss: 0.2182\n",
      "Epoch [1/400], Step [100/213], Loss: 0.1896\n",
      "Epoch [1/400], Step [110/213], Loss: 0.3334\n",
      "Epoch [1/400], Step [120/213], Loss: 0.3010\n",
      "Epoch [1/400], Step [130/213], Loss: 0.2211\n",
      "Epoch [1/400], Step [140/213], Loss: 0.1551\n",
      "Epoch [1/400], Step [150/213], Loss: 0.5036\n",
      "Epoch [1/400], Step [160/213], Loss: 0.1395\n",
      "Epoch [1/400], Step [170/213], Loss: 0.3048\n",
      "Epoch [1/400], Step [180/213], Loss: 0.2300\n",
      "Epoch [1/400], Step [190/213], Loss: 0.2855\n",
      "Epoch [1/400], Step [200/213], Loss: 0.2224\n",
      "Epoch [1/400], Step [210/213], Loss: 0.2734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:13<00:00,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/400] Validation Accuracy: 0.9160\n",
      "Epoch [2/400], Step [10/213], Loss: 0.1945\n",
      "Epoch [2/400], Step [20/213], Loss: 0.0991\n",
      "Epoch [2/400], Step [30/213], Loss: 0.1677\n",
      "Epoch [2/400], Step [40/213], Loss: 0.2006\n",
      "Epoch [2/400], Step [50/213], Loss: 0.2458\n",
      "Epoch [2/400], Step [60/213], Loss: 0.2479\n",
      "Epoch [2/400], Step [70/213], Loss: 0.1665\n",
      "Epoch [2/400], Step [80/213], Loss: 0.2897\n",
      "Epoch [2/400], Step [90/213], Loss: 0.2025\n",
      "Epoch [2/400], Step [100/213], Loss: 0.3231\n",
      "Epoch [2/400], Step [110/213], Loss: 0.2127\n",
      "Epoch [2/400], Step [120/213], Loss: 0.1730\n",
      "Epoch [2/400], Step [130/213], Loss: 0.2556\n",
      "Epoch [2/400], Step [140/213], Loss: 0.4395\n",
      "Epoch [2/400], Step [150/213], Loss: 0.3377\n",
      "Epoch [2/400], Step [160/213], Loss: 0.1409\n",
      "Epoch [2/400], Step [170/213], Loss: 0.1532\n",
      "Epoch [2/400], Step [180/213], Loss: 0.2074\n",
      "Epoch [2/400], Step [190/213], Loss: 0.0988\n",
      "Epoch [2/400], Step [200/213], Loss: 0.1294\n",
      "Epoch [2/400], Step [210/213], Loss: 0.0997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:13<00:00,  4.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/400] Validation Accuracy: 0.8979\n",
      "Epoch [3/400], Step [10/213], Loss: 0.1188\n",
      "Epoch [3/400], Step [20/213], Loss: 0.1742\n",
      "Epoch [3/400], Step [30/213], Loss: 0.1341\n",
      "Epoch [3/400], Step [40/213], Loss: 0.2538\n",
      "Epoch [3/400], Step [50/213], Loss: 0.1759\n",
      "Epoch [3/400], Step [60/213], Loss: 0.3366\n",
      "Epoch [3/400], Step [70/213], Loss: 0.2801\n",
      "Epoch [3/400], Step [80/213], Loss: 0.1236\n",
      "Epoch [3/400], Step [90/213], Loss: 0.2401\n",
      "Epoch [3/400], Step [100/213], Loss: 0.1453\n",
      "Epoch [3/400], Step [110/213], Loss: 0.0829\n",
      "Epoch [3/400], Step [120/213], Loss: 0.2364\n",
      "Epoch [3/400], Step [130/213], Loss: 0.1724\n",
      "Epoch [3/400], Step [140/213], Loss: 0.0853\n",
      "Epoch [3/400], Step [150/213], Loss: 0.1240\n",
      "Epoch [3/400], Step [160/213], Loss: 0.1341\n",
      "Epoch [3/400], Step [170/213], Loss: 0.2044\n",
      "Epoch [3/400], Step [180/213], Loss: 0.2015\n",
      "Epoch [3/400], Step [190/213], Loss: 0.0971\n",
      "Epoch [3/400], Step [200/213], Loss: 0.3246\n",
      "Epoch [3/400], Step [210/213], Loss: 0.3627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:13<00:00,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/400] Validation Accuracy: 0.9295\n",
      "Epoch [4/400], Step [10/213], Loss: 0.0981\n",
      "Epoch [4/400], Step [20/213], Loss: 0.1626\n",
      "Epoch [4/400], Step [30/213], Loss: 0.1158\n",
      "Epoch [4/400], Step [40/213], Loss: 0.1139\n",
      "Epoch [4/400], Step [50/213], Loss: 0.2616\n",
      "Epoch [4/400], Step [60/213], Loss: 0.0821\n",
      "Epoch [4/400], Step [70/213], Loss: 0.1285\n",
      "Epoch [4/400], Step [80/213], Loss: 0.5139\n",
      "Epoch [4/400], Step [90/213], Loss: 0.4061\n",
      "Epoch [4/400], Step [100/213], Loss: 0.1931\n",
      "Epoch [4/400], Step [110/213], Loss: 0.3040\n",
      "Epoch [4/400], Step [120/213], Loss: 0.1070\n",
      "Epoch [4/400], Step [130/213], Loss: 0.0936\n",
      "Epoch [4/400], Step [140/213], Loss: 0.1552\n",
      "Epoch [4/400], Step [150/213], Loss: 0.2354\n",
      "Epoch [4/400], Step [160/213], Loss: 0.0799\n",
      "Epoch [4/400], Step [170/213], Loss: 0.2658\n",
      "Epoch [4/400], Step [180/213], Loss: 0.0835\n",
      "Epoch [4/400], Step [190/213], Loss: 0.0682\n",
      "Epoch [4/400], Step [200/213], Loss: 0.1521\n",
      "Epoch [4/400], Step [210/213], Loss: 0.2811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:13<00:00,  4.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/400] Validation Accuracy: 0.9212\n",
      "Epoch [5/400], Step [10/213], Loss: 0.0387\n",
      "Epoch [5/400], Step [20/213], Loss: 0.1332\n",
      "Epoch [5/400], Step [30/213], Loss: 0.0719\n",
      "Epoch [5/400], Step [40/213], Loss: 0.2544\n",
      "Epoch [5/400], Step [50/213], Loss: 0.4358\n",
      "Epoch [5/400], Step [60/213], Loss: 0.2765\n",
      "Epoch [5/400], Step [70/213], Loss: 0.2152\n",
      "Epoch [5/400], Step [80/213], Loss: 0.0259\n",
      "Epoch [5/400], Step [90/213], Loss: 0.1612\n",
      "Epoch [5/400], Step [100/213], Loss: 0.1747\n",
      "Epoch [5/400], Step [110/213], Loss: 0.0622\n",
      "Epoch [5/400], Step [120/213], Loss: 0.1153\n",
      "Epoch [5/400], Step [130/213], Loss: 0.0777\n",
      "Epoch [5/400], Step [140/213], Loss: 0.1385\n",
      "Epoch [5/400], Step [150/213], Loss: 0.0991\n",
      "Epoch [5/400], Step [160/213], Loss: 0.2675\n",
      "Epoch [5/400], Step [170/213], Loss: 0.3063\n",
      "Epoch [5/400], Step [180/213], Loss: 0.1733\n",
      "Epoch [5/400], Step [190/213], Loss: 0.0807\n",
      "Epoch [5/400], Step [200/213], Loss: 0.0937\n",
      "Epoch [5/400], Step [210/213], Loss: 0.3153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:13<00:00,  4.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/400] Validation Accuracy: 0.9316\n",
      "Epoch [6/400], Step [10/213], Loss: 0.0411\n",
      "Epoch [6/400], Step [20/213], Loss: 0.1221\n",
      "Epoch [6/400], Step [30/213], Loss: 0.1515\n",
      "Epoch [6/400], Step [40/213], Loss: 0.1563\n",
      "Epoch [6/400], Step [50/213], Loss: 0.2683\n",
      "Epoch [6/400], Step [60/213], Loss: 0.0438\n",
      "Epoch [6/400], Step [70/213], Loss: 0.1784\n",
      "Epoch [6/400], Step [80/213], Loss: 0.0948\n",
      "Epoch [6/400], Step [90/213], Loss: 0.0447\n",
      "Epoch [6/400], Step [100/213], Loss: 0.2085\n",
      "Epoch [6/400], Step [110/213], Loss: 0.2652\n",
      "Epoch [6/400], Step [120/213], Loss: 0.1346\n",
      "Epoch [6/400], Step [130/213], Loss: 0.1303\n",
      "Epoch [6/400], Step [140/213], Loss: 0.1529\n",
      "Epoch [6/400], Step [150/213], Loss: 0.0737\n",
      "Epoch [6/400], Step [160/213], Loss: 0.1404\n",
      "Epoch [6/400], Step [170/213], Loss: 0.2292\n",
      "Epoch [6/400], Step [180/213], Loss: 0.0988\n",
      "Epoch [6/400], Step [190/213], Loss: 0.0769\n",
      "Epoch [6/400], Step [200/213], Loss: 0.0730\n",
      "Epoch [6/400], Step [210/213], Loss: 0.1090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:13<00:00,  4.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/400] Validation Accuracy: 0.9269\n",
      "Epoch [7/400], Step [10/213], Loss: 0.2317\n",
      "Epoch [7/400], Step [20/213], Loss: 0.0479\n",
      "Epoch [7/400], Step [30/213], Loss: 0.0464\n",
      "Epoch [7/400], Step [40/213], Loss: 0.1298\n",
      "Epoch [7/400], Step [50/213], Loss: 0.2457\n",
      "Epoch [7/400], Step [60/213], Loss: 0.1299\n",
      "Epoch [7/400], Step [70/213], Loss: 0.2764\n",
      "Epoch [7/400], Step [80/213], Loss: 0.2219\n",
      "Epoch [7/400], Step [90/213], Loss: 0.1464\n",
      "Epoch [7/400], Step [100/213], Loss: 0.1600\n",
      "Epoch [7/400], Step [110/213], Loss: 0.2197\n",
      "Epoch [7/400], Step [120/213], Loss: 0.1265\n",
      "Epoch [7/400], Step [130/213], Loss: 0.0911\n",
      "Epoch [7/400], Step [140/213], Loss: 0.2379\n",
      "Epoch [7/400], Step [150/213], Loss: 0.0466\n",
      "Epoch [7/400], Step [160/213], Loss: 0.2267\n",
      "Epoch [7/400], Step [170/213], Loss: 0.0542\n",
      "Epoch [7/400], Step [180/213], Loss: 0.1571\n",
      "Epoch [7/400], Step [190/213], Loss: 0.0500\n",
      "Epoch [7/400], Step [200/213], Loss: 0.1859\n",
      "Epoch [7/400], Step [210/213], Loss: 0.1258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:13<00:00,  4.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/400] Validation Accuracy: 0.9253\n",
      "Epoch [8/400], Step [10/213], Loss: 0.1153\n",
      "Epoch [8/400], Step [20/213], Loss: 0.1427\n",
      "Epoch [8/400], Step [30/213], Loss: 0.0304\n",
      "Epoch [8/400], Step [40/213], Loss: 0.1252\n",
      "Epoch [8/400], Step [50/213], Loss: 0.2752\n",
      "Epoch [8/400], Step [60/213], Loss: 0.1000\n",
      "Epoch [8/400], Step [70/213], Loss: 0.1970\n",
      "Epoch [8/400], Step [80/213], Loss: 0.1167\n",
      "Epoch [8/400], Step [90/213], Loss: 0.1043\n",
      "Epoch [8/400], Step [100/213], Loss: 0.2243\n",
      "Epoch [8/400], Step [110/213], Loss: 0.1486\n",
      "Epoch [8/400], Step [120/213], Loss: 0.0124\n",
      "Epoch [8/400], Step [130/213], Loss: 0.0732\n",
      "Epoch [8/400], Step [140/213], Loss: 0.1236\n",
      "Epoch [8/400], Step [150/213], Loss: 0.0726\n",
      "Epoch [8/400], Step [160/213], Loss: 0.0514\n",
      "Epoch [8/400], Step [170/213], Loss: 0.0646\n",
      "Epoch [8/400], Step [180/213], Loss: 0.0771\n",
      "Epoch [8/400], Step [190/213], Loss: 0.0464\n",
      "Epoch [8/400], Step [200/213], Loss: 0.0819\n",
      "Epoch [8/400], Step [210/213], Loss: 0.1785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:13<00:00,  4.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/400] Validation Accuracy: 0.9207\n",
      "Epoch [9/400], Step [10/213], Loss: 0.2004\n",
      "Epoch [9/400], Step [20/213], Loss: 0.0356\n",
      "Epoch [9/400], Step [30/213], Loss: 0.0899\n",
      "Epoch [9/400], Step [40/213], Loss: 0.1588\n",
      "Epoch [9/400], Step [50/213], Loss: 0.1443\n",
      "Epoch [9/400], Step [60/213], Loss: 0.1993\n",
      "Epoch [9/400], Step [70/213], Loss: 0.0545\n",
      "Epoch [9/400], Step [80/213], Loss: 0.1408\n",
      "Epoch [9/400], Step [90/213], Loss: 0.0267\n",
      "Epoch [9/400], Step [100/213], Loss: 0.0607\n",
      "Epoch [9/400], Step [110/213], Loss: 0.0817\n",
      "Epoch [9/400], Step [120/213], Loss: 0.0896\n",
      "Epoch [9/400], Step [130/213], Loss: 0.3660\n",
      "Epoch [9/400], Step [140/213], Loss: 0.1885\n",
      "Epoch [9/400], Step [150/213], Loss: 0.0478\n",
      "Epoch [9/400], Step [160/213], Loss: 0.1151\n",
      "Epoch [9/400], Step [170/213], Loss: 0.0207\n",
      "Epoch [9/400], Step [180/213], Loss: 0.1318\n",
      "Epoch [9/400], Step [190/213], Loss: 0.1763\n",
      "Epoch [9/400], Step [200/213], Loss: 0.0464\n",
      "Epoch [9/400], Step [210/213], Loss: 0.0708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:13<00:00,  4.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/400] Validation Accuracy: 0.9248\n",
      "Epoch [10/400], Step [10/213], Loss: 0.0927\n",
      "Epoch [10/400], Step [20/213], Loss: 0.1244\n",
      "Epoch [10/400], Step [30/213], Loss: 0.1368\n",
      "Epoch [10/400], Step [40/213], Loss: 0.0518\n",
      "Epoch [10/400], Step [50/213], Loss: 0.0284\n",
      "Epoch [10/400], Step [60/213], Loss: 0.1651\n",
      "Epoch [10/400], Step [70/213], Loss: 0.1515\n",
      "Epoch [10/400], Step [80/213], Loss: 0.2211\n",
      "Epoch [10/400], Step [90/213], Loss: 0.1378\n",
      "Epoch [10/400], Step [100/213], Loss: 0.0793\n",
      "Epoch [10/400], Step [110/213], Loss: 0.0461\n",
      "Epoch [10/400], Step [120/213], Loss: 0.2128\n",
      "Epoch [10/400], Step [130/213], Loss: 0.1913\n",
      "Epoch [10/400], Step [140/213], Loss: 0.0745\n",
      "Epoch [10/400], Step [150/213], Loss: 0.1775\n",
      "Epoch [10/400], Step [160/213], Loss: 0.0511\n",
      "Epoch [10/400], Step [170/213], Loss: 0.0498\n",
      "Epoch [10/400], Step [180/213], Loss: 0.0645\n",
      "Epoch [10/400], Step [190/213], Loss: 0.0922\n",
      "Epoch [10/400], Step [200/213], Loss: 0.0830\n",
      "Epoch [10/400], Step [210/213], Loss: 0.0286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:13<00:00,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/400] Validation Accuracy: 0.9290\n",
      "Epoch [11/400], Step [10/213], Loss: 0.0667\n",
      "Epoch [11/400], Step [20/213], Loss: 0.2496\n",
      "Epoch [11/400], Step [30/213], Loss: 0.1694\n",
      "Epoch [11/400], Step [40/213], Loss: 0.0427\n",
      "Epoch [11/400], Step [50/213], Loss: 0.0769\n",
      "Epoch [11/400], Step [60/213], Loss: 0.0855\n",
      "Epoch [11/400], Step [70/213], Loss: 0.1030\n",
      "Epoch [11/400], Step [80/213], Loss: 0.0605\n",
      "Epoch [11/400], Step [90/213], Loss: 0.0908\n",
      "Epoch [11/400], Step [100/213], Loss: 0.0445\n",
      "Epoch [11/400], Step [110/213], Loss: 0.0295\n",
      "Epoch [11/400], Step [120/213], Loss: 0.0786\n",
      "Epoch [11/400], Step [130/213], Loss: 0.2304\n",
      "Epoch [11/400], Step [140/213], Loss: 0.1054\n",
      "Epoch [11/400], Step [150/213], Loss: 0.1164\n",
      "Epoch [11/400], Step [160/213], Loss: 0.0823\n",
      "Epoch [11/400], Step [170/213], Loss: 0.0525\n",
      "Epoch [11/400], Step [180/213], Loss: 0.1091\n",
      "Epoch [11/400], Step [190/213], Loss: 0.1448\n",
      "Epoch [11/400], Step [200/213], Loss: 0.1527\n",
      "Epoch [11/400], Step [210/213], Loss: 0.2366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:13<00:00,  4.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/400] Validation Accuracy: 0.9222\n",
      "Epoch [12/400], Step [10/213], Loss: 0.0356\n",
      "Epoch [12/400], Step [20/213], Loss: 0.0549\n",
      "Epoch [12/400], Step [30/213], Loss: 0.1711\n",
      "Epoch [12/400], Step [40/213], Loss: 0.1379\n",
      "Epoch [12/400], Step [50/213], Loss: 0.0665\n",
      "Epoch [12/400], Step [60/213], Loss: 0.0148\n",
      "Epoch [12/400], Step [70/213], Loss: 0.4212\n",
      "Epoch [12/400], Step [80/213], Loss: 0.0248\n",
      "Epoch [12/400], Step [90/213], Loss: 0.1605\n",
      "Epoch [12/400], Step [100/213], Loss: 0.1190\n",
      "Epoch [12/400], Step [110/213], Loss: 0.2985\n",
      "Epoch [12/400], Step [120/213], Loss: 0.0999\n",
      "Epoch [12/400], Step [130/213], Loss: 0.0315\n",
      "Epoch [12/400], Step [140/213], Loss: 0.0450\n",
      "Epoch [12/400], Step [150/213], Loss: 0.1080\n",
      "Epoch [12/400], Step [160/213], Loss: 0.1532\n",
      "Epoch [12/400], Step [170/213], Loss: 0.0138\n",
      "Epoch [12/400], Step [180/213], Loss: 0.0728\n",
      "Epoch [12/400], Step [190/213], Loss: 0.1033\n",
      "Epoch [12/400], Step [200/213], Loss: 0.1038\n",
      "Epoch [12/400], Step [210/213], Loss: 0.2679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:13<00:00,  4.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/400] Validation Accuracy: 0.9222\n",
      "Epoch [13/400], Step [10/213], Loss: 0.0763\n",
      "Epoch [13/400], Step [20/213], Loss: 0.0508\n",
      "Epoch [13/400], Step [30/213], Loss: 0.0303\n",
      "Epoch [13/400], Step [40/213], Loss: 0.1611\n",
      "Epoch [13/400], Step [50/213], Loss: 0.0288\n",
      "Epoch [13/400], Step [60/213], Loss: 0.0708\n",
      "Epoch [13/400], Step [70/213], Loss: 0.1549\n",
      "Epoch [13/400], Step [80/213], Loss: 0.0788\n",
      "Epoch [13/400], Step [90/213], Loss: 0.0924\n",
      "Epoch [13/400], Step [100/213], Loss: 0.1281\n",
      "Epoch [13/400], Step [110/213], Loss: 0.0728\n",
      "Epoch [13/400], Step [120/213], Loss: 0.0090\n",
      "Epoch [13/400], Step [130/213], Loss: 0.0870\n",
      "Epoch [13/400], Step [140/213], Loss: 0.0656\n",
      "Epoch [13/400], Step [150/213], Loss: 0.1519\n",
      "Epoch [13/400], Step [160/213], Loss: 0.0901\n",
      "Epoch [13/400], Step [170/213], Loss: 0.0109\n",
      "Epoch [13/400], Step [180/213], Loss: 0.2576\n",
      "Epoch [13/400], Step [190/213], Loss: 0.2005\n",
      "Epoch [13/400], Step [200/213], Loss: 0.0940\n",
      "Epoch [13/400], Step [210/213], Loss: 0.1014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:13<00:00,  4.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/400] Validation Accuracy: 0.9285\n",
      "Epoch [14/400], Step [10/213], Loss: 0.2033\n",
      "Epoch [14/400], Step [20/213], Loss: 0.0779\n",
      "Epoch [14/400], Step [30/213], Loss: 0.0204\n",
      "Epoch [14/400], Step [40/213], Loss: 0.0716\n",
      "Epoch [14/400], Step [50/213], Loss: 0.0598\n",
      "Epoch [14/400], Step [60/213], Loss: 0.0636\n",
      "Epoch [14/400], Step [70/213], Loss: 0.2213\n",
      "Epoch [14/400], Step [80/213], Loss: 0.1425\n",
      "Epoch [14/400], Step [90/213], Loss: 0.0434\n",
      "Epoch [14/400], Step [100/213], Loss: 0.1575\n",
      "Epoch [14/400], Step [110/213], Loss: 0.0302\n",
      "Epoch [14/400], Step [120/213], Loss: 0.0222\n",
      "Epoch [14/400], Step [130/213], Loss: 0.1738\n",
      "Epoch [14/400], Step [140/213], Loss: 0.1325\n",
      "Epoch [14/400], Step [150/213], Loss: 0.0749\n",
      "Epoch [14/400], Step [160/213], Loss: 0.0173\n",
      "Epoch [14/400], Step [170/213], Loss: 0.0226\n",
      "Epoch [14/400], Step [180/213], Loss: 0.0562\n",
      "Epoch [14/400], Step [190/213], Loss: 0.1882\n",
      "Epoch [14/400], Step [200/213], Loss: 0.0492\n",
      "Epoch [14/400], Step [210/213], Loss: 0.1467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:13<00:00,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/400] Validation Accuracy: 0.9253\n",
      "Epoch [15/400], Step [10/213], Loss: 0.0195\n",
      "Epoch [15/400], Step [20/213], Loss: 0.1527\n",
      "Epoch [15/400], Step [30/213], Loss: 0.0106\n",
      "Epoch [15/400], Step [40/213], Loss: 0.0640\n",
      "Epoch [15/400], Step [50/213], Loss: 0.0125\n",
      "Epoch [15/400], Step [60/213], Loss: 0.0725\n",
      "Epoch [15/400], Step [70/213], Loss: 0.0916\n",
      "Epoch [15/400], Step [80/213], Loss: 0.0179\n",
      "Epoch [15/400], Step [90/213], Loss: 0.1380\n",
      "Epoch [15/400], Step [100/213], Loss: 0.0582\n",
      "Epoch [15/400], Step [110/213], Loss: 0.1044\n",
      "Epoch [15/400], Step [120/213], Loss: 0.0523\n",
      "Epoch [15/400], Step [130/213], Loss: 0.1372\n",
      "Epoch [15/400], Step [140/213], Loss: 0.0831\n",
      "Epoch [15/400], Step [150/213], Loss: 0.0520\n",
      "Epoch [15/400], Step [160/213], Loss: 0.0103\n",
      "Epoch [15/400], Step [170/213], Loss: 0.1906\n",
      "Epoch [15/400], Step [180/213], Loss: 0.0009\n",
      "Epoch [15/400], Step [190/213], Loss: 0.0348\n",
      "Epoch [15/400], Step [200/213], Loss: 0.1149\n",
      "Epoch [15/400], Step [210/213], Loss: 0.4052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:13<00:00,  4.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/400] Validation Accuracy: 0.9098\n",
      "Epoch [16/400], Step [10/213], Loss: 0.2592\n",
      "Epoch [16/400], Step [20/213], Loss: 0.0342\n",
      "Epoch [16/400], Step [30/213], Loss: 0.0852\n",
      "Epoch [16/400], Step [40/213], Loss: 0.0125\n",
      "Epoch [16/400], Step [50/213], Loss: 0.0924\n",
      "Epoch [16/400], Step [60/213], Loss: 0.0665\n",
      "Epoch [16/400], Step [70/213], Loss: 0.0665\n",
      "Epoch [16/400], Step [80/213], Loss: 0.1510\n",
      "Epoch [16/400], Step [90/213], Loss: 0.0614\n",
      "Epoch [16/400], Step [100/213], Loss: 0.1650\n",
      "Epoch [16/400], Step [110/213], Loss: 0.1040\n",
      "Epoch [16/400], Step [120/213], Loss: 0.1541\n",
      "Epoch [16/400], Step [130/213], Loss: 0.0115\n",
      "Epoch [16/400], Step [140/213], Loss: 0.1149\n",
      "Epoch [16/400], Step [150/213], Loss: 0.0787\n",
      "Epoch [16/400], Step [160/213], Loss: 0.0603\n",
      "Epoch [16/400], Step [170/213], Loss: 0.0635\n",
      "Epoch [16/400], Step [180/213], Loss: 0.0334\n",
      "Epoch [16/400], Step [190/213], Loss: 0.1313\n",
      "Epoch [16/400], Step [200/213], Loss: 0.0662\n",
      "Epoch [16/400], Step [210/213], Loss: 0.0546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:13<00:00,  4.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/400] Validation Accuracy: 0.9243\n",
      "Epoch [17/400], Step [10/213], Loss: 0.0809\n",
      "Epoch [17/400], Step [20/213], Loss: 0.1118\n",
      "Epoch [17/400], Step [30/213], Loss: 0.0637\n",
      "Epoch [17/400], Step [40/213], Loss: 0.0353\n",
      "Epoch [17/400], Step [50/213], Loss: 0.0614\n",
      "Epoch [17/400], Step [60/213], Loss: 0.0321\n",
      "Epoch [17/400], Step [70/213], Loss: 0.1235\n",
      "Epoch [17/400], Step [80/213], Loss: 0.2810\n",
      "Epoch [17/400], Step [90/213], Loss: 0.0582\n",
      "Epoch [17/400], Step [100/213], Loss: 0.0258\n",
      "Epoch [17/400], Step [110/213], Loss: 0.1050\n",
      "Epoch [17/400], Step [120/213], Loss: 0.0214\n",
      "Epoch [17/400], Step [130/213], Loss: 0.1971\n",
      "Epoch [17/400], Step [140/213], Loss: 0.0329\n",
      "Epoch [17/400], Step [150/213], Loss: 0.0877\n",
      "Epoch [17/400], Step [160/213], Loss: 0.1038\n",
      "Epoch [17/400], Step [170/213], Loss: 0.0125\n",
      "Epoch [17/400], Step [180/213], Loss: 0.1071\n",
      "Epoch [17/400], Step [190/213], Loss: 0.0677\n",
      "Epoch [17/400], Step [200/213], Loss: 0.0672\n",
      "Epoch [17/400], Step [210/213], Loss: 0.0352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:13<00:00,  4.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/400] Validation Accuracy: 0.9243\n",
      "Epoch [18/400], Step [10/213], Loss: 0.0475\n",
      "Epoch [18/400], Step [20/213], Loss: 0.0095\n",
      "Epoch [18/400], Step [30/213], Loss: 0.1552\n",
      "Epoch [18/400], Step [40/213], Loss: 0.1455\n",
      "Epoch [18/400], Step [50/213], Loss: 0.0353\n",
      "Epoch [18/400], Step [60/213], Loss: 0.0270\n",
      "Epoch [18/400], Step [70/213], Loss: 0.0652\n",
      "Epoch [18/400], Step [80/213], Loss: 0.1042\n",
      "Epoch [18/400], Step [90/213], Loss: 0.0671\n",
      "Epoch [18/400], Step [100/213], Loss: 0.0010\n",
      "Epoch [18/400], Step [110/213], Loss: 0.1244\n",
      "Epoch [18/400], Step [120/213], Loss: 0.0283\n",
      "Epoch [18/400], Step [130/213], Loss: 0.1755\n",
      "Epoch [18/400], Step [140/213], Loss: 0.0156\n",
      "Epoch [18/400], Step [150/213], Loss: 0.0607\n",
      "Epoch [18/400], Step [160/213], Loss: 0.1087\n",
      "Epoch [18/400], Step [170/213], Loss: 0.1682\n",
      "Epoch [18/400], Step [180/213], Loss: 0.0951\n",
      "Epoch [18/400], Step [190/213], Loss: 0.1952\n",
      "Epoch [18/400], Step [200/213], Loss: 0.1702\n",
      "Epoch [18/400], Step [210/213], Loss: 0.0882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:13<00:00,  4.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/400] Validation Accuracy: 0.9228\n",
      "Epoch [19/400], Step [10/213], Loss: 0.1810\n",
      "Epoch [19/400], Step [20/213], Loss: 0.0540\n",
      "Epoch [19/400], Step [30/213], Loss: 0.0107\n",
      "Epoch [19/400], Step [40/213], Loss: 0.0455\n",
      "Epoch [19/400], Step [50/213], Loss: 0.1461\n",
      "Epoch [19/400], Step [60/213], Loss: 0.1280\n",
      "Epoch [19/400], Step [70/213], Loss: 0.0813\n",
      "Epoch [19/400], Step [80/213], Loss: 0.2084\n",
      "Epoch [19/400], Step [90/213], Loss: 0.0589\n",
      "Epoch [19/400], Step [100/213], Loss: 0.0477\n",
      "Epoch [19/400], Step [110/213], Loss: 0.0437\n",
      "Epoch [19/400], Step [120/213], Loss: 0.0921\n",
      "Epoch [19/400], Step [130/213], Loss: 0.1261\n",
      "Epoch [19/400], Step [140/213], Loss: 0.0376\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     12\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     14\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchange_labels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# 零梯度\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/geochat/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/geochat/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/geochat/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/geochat/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[4], line 49\u001b[0m, in \u001b[0;36mLazySupervisedDataset.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     39\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\n\u001b[1;32m     40\u001b[0m         (os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_folder, _image_file))\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     41\u001b[0m     )\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m     image \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mpreprocess(\n\u001b[1;32m     43\u001b[0m         image,\n\u001b[1;32m     44\u001b[0m         do_resize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m     )[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mimageList\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# 将多幅图像拼成一个Tensor\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "num_epochs = 400\n",
    "print_step = 10\n",
    "global_step = 0\n",
    "output_dir = \"/root/autodl-tmp/GeoChat/experiments/classifier\"\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = output_dir + now\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        images, labels = batch['images'].to(device), batch['change_labels'].to(device)\n",
    "        \n",
    "        # 零梯度\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "        global_step += 1\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (i+1)%print_step == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"global_step\": global_step,\n",
    "                \"loss\": loss.item()\n",
    "            })\n",
    "    \n",
    "    # 打印每个epoch的平均损失\n",
    "    val_accuracy = calculate_accuracy(model, test_loader, device)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    torch.save(model.classifier.state_dict(), os.path.join(output_dir,\"classifier_weight.pth\"))\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"val_accuracy\": val_accuracy,\n",
    "    })\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geochat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
