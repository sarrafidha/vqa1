{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/root/autodl-tmp/CLIP\")\n",
    "import clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "device = torch.device(\"cuda\")\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, img_feature_h = 7, img_feature_w = 7):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.img_feature_h = img_feature_h\n",
    "        self.img_feature_w = img_feature_w\n",
    "\n",
    "        self.gpt_embedding_size = 768  # self.gpt.transformer.wte.weight.shape[1]\n",
    "\n",
    "        self.d_model = self.gpt_embedding_size\n",
    "        # position embedding：\n",
    "        l = 50\n",
    "        self.l_embedding = nn.Embedding(l, int(self.d_model))\n",
    "\n",
    "        self.w_embedding = nn.Embedding(img_feature_w, int(self.d_model / 2))\n",
    "        self.h_embedding = nn.Embedding(img_feature_h, int(self.d_model / 2))\n",
    "        self.temporal_embedding = nn.Embedding(2, int(self.d_model))\n",
    "\n",
    "        encoder_self_layer = nn.TransformerEncoderLayer(1 * self.d_model, nhead=8,\n",
    "                                                        dim_feedforward=int(4 * self.d_model))\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_self_layer, num_layers=2)\n",
    "\n",
    "        encoder_self_layer_classifier = nn.TransformerEncoderLayer(2 * self.d_model, nhead=8,\n",
    "                                                                   dim_feedforward=int(4 * self.d_model))\n",
    "        self.transformer_encoder_classifier = nn.TransformerEncoder(encoder_self_layer_classifier, num_layers=3)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(self.d_model, nhead=8, dim_feedforward=self.d_model * 2)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, 1)\n",
    "\n",
    "        self.classifier_projection = nn.Linear(2 * self.d_model, 2)\n",
    "\n",
    "        # cls_token\n",
    "        scale = self.d_model ** -0.5\n",
    "        self.class_embedding_classifier_changeflag = nn.Parameter(scale * torch.randn(1, 2 * self.d_model))\n",
    "        # cls_token\n",
    "        self.class_embedding_A = nn.Parameter(scale * torch.randn(1, self.d_model))\n",
    "        self.class_embedding_B = nn.Parameter(scale * torch.randn(1, self.d_model))\n",
    "\n",
    "\n",
    "        self.conv_dif = nn.Sequential(\n",
    "            nn.Conv2d(self.d_model, int(self.d_model / 2), kernel_size=3),\n",
    "            # nn.LayerNorm(int(outchannel/2),dim=1),\n",
    "            nn.BatchNorm2d(int(self.d_model / 2)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveMaxPool2d((1, 1))\n",
    "        )\n",
    "        self.linear_dif = nn.Linear(int(self.d_model / 2), 2)\n",
    "\n",
    "        self.pre_linear = nn.Linear(self.gpt_embedding_size, self.d_model)\n",
    "\n",
    "\n",
    "    def position_embedding_1D_func(self, embedding_text):\n",
    "        batch = embedding_text.shape[0]\n",
    "        Len_feat = embedding_text.shape[1]\n",
    "\n",
    "        pos_l = torch.arange(Len_feat, device=device).to(device)\n",
    "\n",
    "        position_embedding = self.l_embedding(pos_l)\n",
    "\n",
    "        position_embedding = position_embedding.unsqueeze(0).repeat(batch, 1, 1, 1)  # (batch, l, d_model)\n",
    "        position_embedding = position_embedding.view(batch, -1, self.d_model)\n",
    "        embedding_text = embedding_text + position_embedding  # NLD\n",
    "\n",
    "        return embedding_text\n",
    "\n",
    "    def position_embedding_2D_func(self, img_feat_A, img_feat_B):\n",
    "        batch = img_feat_B.shape[0]\n",
    "        Len_feat = img_feat_B.shape[1]\n",
    "        h = int(math.sqrt(Len_feat))\n",
    "        w = h\n",
    "        pos_w = torch.arange(w, device=device).to(device)\n",
    "        pos_h = torch.arange(h, device=device).to(device)\n",
    "        embed_w = self.w_embedding(pos_w)\n",
    "        embed_h = self.h_embedding(pos_h)\n",
    "        position_embedding = torch.cat([embed_w.unsqueeze(0).repeat(h, 1, 1),\n",
    "                                        embed_h.unsqueeze(1).repeat(1, w, 1)],\n",
    "                                       dim=-1)\n",
    "        position_embedding = position_embedding.unsqueeze(0).repeat(batch, 1, 1, 1)  # (batch, h, w, d_model)\n",
    "        position_embedding = position_embedding.view(batch, -1, self.d_model)\n",
    "        img_feat_A = img_feat_A + position_embedding  # NLD\n",
    "        img_feat_B = img_feat_B + position_embedding  # NLD\n",
    "\n",
    "        return img_feat_A, img_feat_B\n",
    "\n",
    "    def Siamese_bridge_net(self, class_embedding, img_feat):\n",
    "        conc_A = torch.cat(\n",
    "            [class_embedding.unsqueeze(0).expand(img_feat.shape[0], *class_embedding.shape),\n",
    "             img_feat], dim=1)\n",
    "        conc_A = self.transformer_encoder(conc_A.permute(1, 0, 2)).permute(1, 0, 2)  # NLD\n",
    "        cls_A = conc_A[:, 0, :]  # self.cls_projection(conc_A[:, 0, :])\n",
    "        img_refine = conc_A[:, 1:, :]  # NLD\n",
    "        return cls_A, img_refine\n",
    "\n",
    "    def Classifier(self, img_feat):\n",
    "        # img_feat = self.pre_linear(img_feat)\n",
    "        img_feat_A = img_feat[:, 0, ...]  # (N,L,768)\n",
    "        img_feat_B = img_feat[:, 1, ...]\n",
    "        batch = img_feat_B.shape[0]\n",
    "        Len_feat = img_feat_B.shape[1]\n",
    "        h = int(math.sqrt(Len_feat))\n",
    "\n",
    "\n",
    "        # # # 2D image position_embedding\n",
    "        img_feat_A, img_feat_B = self.position_embedding_2D_func(img_feat_A, img_feat_B)  # NLD\n",
    "\n",
    "        img_feat = img_feat_B - img_feat_A  # torch.abs(img_feat_B-img_feat_A)#torch.cat([img_feat_A, img_feat_B],dim=-1)\n",
    "        _, img_feat = self.position_embedding_2D_func(img_feat_A, img_feat)  # NLD\n",
    "\n",
    "        img_feat = torch.cat([img_feat_A, img_feat_B], dim=-1)\n",
    "        conc_A = torch.cat(\n",
    "            [self.class_embedding_classifier_changeflag.unsqueeze(0).expand(img_feat.shape[0],\n",
    "                                                                            *self.class_embedding_classifier_changeflag.shape),\n",
    "             img_feat], dim=1)\n",
    "\n",
    "        conc_A = self.transformer_encoder_classifier(conc_A.permute(1, 0, 2)).permute(1, 0, 2)  # NLD\n",
    "        changeflag = self.classifier_projection(conc_A[:, 0, :])  # self.cls_projection(conc_A[:, 0, :])\n",
    "\n",
    "        return changeflag\n",
    "\n",
    "    def forward(self, featuremap):\n",
    "        # bridge Network\n",
    "        changeflag = self.Classifier(featuremap)\n",
    "        # classifier_pre_flag\n",
    "        return changeflag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ChangeClassifierConfig:\n",
    "    mm_vision_tower = \"ViT-B/32\"\n",
    "\n",
    "class ChangeClassifier(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ChangeClassifier, self).__init__()\n",
    "        device = device=torch.device(\"cuda\")\n",
    "        self.vision_tower, self.preprocess = clip.load(config.mm_vision_tower, device=device, jit=False)\n",
    "        # freeeze vision_tower\n",
    "        self.vision_tower.requires_grad_(False)\n",
    "\n",
    "        self.classifier = Classifier()\n",
    "        ckpt = torch.load(\"/root/autodl-tmp/GeoChat/hf-models/PromptCC/cls_model.pth.tar\")\n",
    "        self.classifier.load_state_dict(ckpt['model_state_dict()'], strict=False)\n",
    "        self.classifier.eval()\n",
    "\n",
    "    def forward(self, images):\n",
    "        # 提取两幅图像的特征\n",
    "        assert images.ndim == 5\n",
    "        concat_images = torch.cat([image for image in images], dim=0)\n",
    "        _, image_features = self.vision_tower.encode_image(concat_images)\n",
    "        split_sizes = [image.shape[0] for image in images]\n",
    "        image_features = torch.split(\n",
    "            image_features, split_sizes, dim=0\n",
    "        )  # b tuples of [2, N, L]\n",
    "        image_features = torch.stack(image_features, dim=0)  # [b, 2, N, L]\n",
    "        change_pred = self.classifier(image_features)\n",
    "        return change_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataConfig:\n",
    "    data_path = \"/root/autodl-tmp/LEVIR-MCI-dataset/ChangeChat_classify.json\"\n",
    "    image_folder = \"/root/autodl-tmp/LEVIR-MCI-dataset/images\"\n",
    "    image_processor = None\n",
    "\n",
    "class LazySupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config\n",
    "    ):\n",
    "        super(LazySupervisedDataset, self).__init__()\n",
    "        list_data_dict = json.load(open(config.data_path, \"r\"))\n",
    "        self.list_data_dict = list_data_dict\n",
    "        self.processor = config.image_processor\n",
    "        self.image_folder = config.image_folder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_data_dict)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        sources = self.list_data_dict[i]\n",
    "        if isinstance(i, int):\n",
    "            sources = [sources]\n",
    "        assert len(sources) == 1, \"Don't know why it is wrapped to a list\"  # FIXME\n",
    "        image_file = self.list_data_dict[i][\"image\"]\n",
    "        image_folder = self.image_folder\n",
    "        processor = self.processor\n",
    "\n",
    "        if isinstance(image_file, str):\n",
    "            image_file_list = [image_file]\n",
    "        elif isinstance(image_file, list):\n",
    "            image_file_list = image_file\n",
    "\n",
    "        imageList = []\n",
    "        for _image_file in image_file_list:\n",
    "            image = Image.open(\n",
    "                (os.path.join(image_folder, _image_file)).strip()\n",
    "            ).convert(\"RGB\")\n",
    "            image = processor(\n",
    "                image,\n",
    "            )\n",
    "            imageList.append(image)\n",
    "\n",
    "        data_dict = dict()\n",
    "        # 将多幅图像拼成一个Tensor\n",
    "        data_dict[\"images\"] = torch.stack(imageList, dim=0)  # (2, c, h, w)\n",
    "\n",
    "        # 如果有变化标签\n",
    "        if \"changeflag\" in self.list_data_dict[i]:\n",
    "            data_dict[\"change_labels\"] = torch.tensor(\n",
    "                self.list_data_dict[i][\"changeflag2\"]\n",
    "            )\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = ChangeClassifierConfig()\n",
    "model = ChangeClassifier(config)\n",
    "model = model.to(device)\n",
    "# 创建数据集和数据加载器\n",
    "dataset_config = DataConfig()\n",
    "dataset_config.image_processor = model.preprocess\n",
    "train_dataset = LazySupervisedDataset(dataset_config)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 验证集\n",
    "dataset_config.data_path = \"/root/autodl-tmp/GeoChat/load/Test_CC_gt.json\"\n",
    "test_dataset = LazySupervisedDataset(dataset_config)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "params = [v for v in model.parameters() if v.requires_grad]\n",
    "num_params = sum([v.numel() for v in model.parameters() if v.requires_grad])/1000000\n",
    "optimizer = optim.Adam(params, lr=1e-4)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def calculate_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predicts_all = []\n",
    "    labels_all = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(data_loader):\n",
    "            images, labels = batch['images'].to(device), batch['change_labels'].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predicts_all.extend(predicted.tolist())\n",
    "            labels_all.extend(labels.tolist())\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy, predicts_all, labels_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:20<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "accuracy, predicts_all, labels_all = calculate_accuracy(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率：0.9160186625194401\n",
      "召回率：0.8578838174273858\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "predicts_all2 = [1 for _ in predicts_all]\n",
    "print(f\"准确率：{accuracy_score(labels_all, predicts_all)}\")\n",
    "print(f\"召回率：{recall_score(labels_all, predicts_all)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geochat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
